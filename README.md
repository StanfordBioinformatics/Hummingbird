# Hummingbird: provide fast and cheap configurations to run genomics applications on Google cloud

## Overview

Hummingbird is a Python framework that gives you a variety of optimum instance configurations to run your favorite genomics pipeline on Google cloud.

The framework takes as input the necessary information required to run a [dsub](https://github.com/DataBiosphere/dsub) job and outputs different instance configurations that the user can use to run his/her pipeline on Google cloud. The user can choose from a variety of instance configurations, such as fastest, cheapest, fastest and cheapest, and fastest in terms of normalized speedup. What these configurations exactly mean will be explained later on in this README.

The unique thing about Hummingbird is that it takes the input files, downsamples them and then runs the whole pipeline on the downwsampled files. This results in users getting the resulting configurations in a short amount of time, compared to the time it would have taken to run the entire pipeline on the whole input file for different configurations and then give the user different optimum configurations.

As of now Hummingbird only supports Google cloud but we hope to add other cloud providers in the future.

## Getting Started

Hummingbird itself does not require any installation, however there are some packages which it uses which need to be installed.

### Pre-requisites

1.  Python 2.7
1.  Google cloud SDK
1.  dsub

For `dsub` please ensure that you execute `source dsub_libs/bin/activate` in your home directory. Also please do not forget to execute `gcloud auth application-default login` else dsub will not be able to launch jobs on Google cloud.

Since Hummingbird requires a working knowledge of dsub, we recommend that you go through the documentation of dsub and try executing your pipeline using dsub, before trying it out with Hummingbird. 

### Cloning this repo

```
git clone https://github.com/Hummingbird
cd Hummingbird
```

### Editing the `User_Provided_Input.txt` file

The `User_Provided_Input.txt` file contains all the information that Hummingbird will need to launch dsub jobs on Google cloud. The format of the text file is very important and adding any unnecessary lines or spaces will throw an error. We will go through each field required in the text file over here:

1.  `--project` is the name of the project which will allow you to use all Google cloud functionalities. Make sure that the current Google account that you specified while doing `gcloud init` has access to the project name you are specifying here.

1.  `--zones` specifies in which zone you would like the dsub to launch Google cloud instances. Keep in mind different zones have different pricing. You can know about the current Google cloud pricing from this [link](https://cloud.google.com/compute/pricing)

1.  `--logging` contains the Google cloud bucket where all the log files generated by dsub will be stored. Please just specify the folder name. There is no need to give a specific name for the log file. Hummingbird uses the default naming convention provided by dsub to look up the log file in the bucket. Providing a specific name for the log file would result in errors.

1.  `--input-recrsive` is the name of the Google cloud bucket which contains the input files that will be required for the pipeline you wish to execute. Please provide a folder name which contains all the relevant files.  Keep in mind, whenever you specify an input file in your command line, you will have to preface that with `"${INPUT_PATH}"/`.

1.  `--output` specifies the folder to which dsub will write the output files. Just like the input files, whenever you specify an output file in your command line, you will have to preface that with `"$(dirname ${OUTPUT_FILE})"`

1.  `--min-ram` is the amount of RAM with which dsub will launch an instance. There is no need to change this field as Hummingbird will try out different instances with different RAM to try and get the best configuration

1.  `--min-cores` is the number of cores with which dsub will launch an instance. Just like `--min-ram` there is no need for you to modify this field

1.  `image` specifies the Docker image on which your pipeline will be executed. 

1.  `--command` is the actual command line which executes either one stage of the pipeline or the entire pipeline. If your pipeline has multiple stages, you can specify multiple `--command` arguments on a new line. If you are specifying multiple stages please make sure that the output file name of one stage matches the input file name of the next stage. 

1.  `Input_File` is the name of the file which will be downsampled. This will typically be a fasta file which contains sequences. Specify each input file in a separate line in the format already given in the text file

1.  `Multi-threaded` specifies whether the command is multi-threaded or not. The number of entires after the `=` should correspond to the number of command lines. If three command lines are given for the pipeline, there should be three arguments in this field. If the command is not multi-threaded a simple `NO` will suffice, but in case the command is multi-threaded please specify the argument that controls the multi-threading option. For example it might be `-t` or `-nct`, etc.

Please keep in mind that the format of the txt file is very important and you should not add any lines unless it is a "--command" line. Please go over the sample txt file we have provided to get an idea of the format to mention input buckets, output buckets, etc.

## Executing Hummingbird

Once the pre-requisites have been installed and the text file has been modified properly, you can execute Hummingbird. Please keep in mind Hummingbird will download your input files, downsample them, and then upload the downsampled files to the Google bucket mentioned in the `--input-recursive` field. So make sure that there is enough space locally to download the input files.

To execute Hummingbird run the following command
```
python framework.py
```

## Result

For each stage of the pipeline, Hummingbird will print 4 different configurations on the terminal: fastest(in terms of execution time), fastest and cheapest, cheapest, and fastest(in terms of normalized speedup). The meaning of each term will be more clear in the following lines.

1.  Fastest(in terms of execution time): This configuration indicates the fastest amongst all the different configurations that Hummingbird executed the pertaining stage of the pipeline on. Here fastest means the configuration with the least execution time.

1.  Fastest and cheapest: There are three different categories of machines in Google cloud, standard, high mem and high CPU. The instance type with the highest normalized speedup in each category is chosen and then those three instance types are compared and the one with the lowest cost is chosen as the "fastest and cheapest" configuration.

1. Cheapest: The instance which costs the least is chosen as the instance with the cheapest configuration.

1. Fastest(in terms of normalized speedup): This configurations indicates the fastest configuration in terms of normalized speedup across all three categories.

To understand how we normalized speedup please read our paper: <insert link here>

## Future Work
